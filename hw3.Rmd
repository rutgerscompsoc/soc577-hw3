---
title: "SOC 577 Homework 3: Natural language processing"
author: "Your name here"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Do not edit this chunk

# The following lines define how the output of code chunks should behave
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = TRUE)

# Required packages, please install any you do not have
library(rmarkdown)
library(tidyverse)
library(knitr)
library(stringr)
library(tidytext)
library(word2vec)
library(stm)
library(ggplot2)
library(viridis)
library(parallel)
library(reshape2)
library(magrittr)
```

# Instructions

This assignment is designed to build your familiarity with the natural language processing techniques covered in class. As in the previous assignments, it will involve a combination of short written answers and coding in R. All answers should be written in this document. *Please write answers to written questions outside of the code cells rather than as comments.*

### Requirements
You should be viewing this document in RStudio. If you have not done so already, make sure to install the required packages (see initial chunk). You can do this by clicking the ``Install`` button in the Packages tab in the lower-right corner of RStudio and following the directions on the installation menu. You can also install packages by entering ``install.packages(x)`` into the R Console, where ``x`` is the name of the package.

### Submitting the homework
Once you have finished the assignment please complete the following steps to submit it:

1. Click on the ``Knit`` menu at the top of the screen and select ``Knit to HTML``. This will execute the all of the code and render the RMarkdown document in HTML. Verify that this document contains all of your answers and that none of the chunks produce error messages.
2. Add this document *and* the HTML file to Github. Use ``Homework submitted`` as your main commit message.
3. Push the commit to Github.
4. Visit the Github repository in your browser and verify that the final version of both files has been correctly uploaded.

# **Part I: From text to vector representations**

There are two different opinions for this assignment. You can either analyze the transcripts of *Friends* or *South Park*. Both datasets are approximately the same with respect to scale and structure. The *Friends* dataset contains 61k lines uttered over all 10 seasons and the *South Park* data contains 70k lines from the first 18 seasons. Please be warned that the *South Park* transcripts contain a lot of a highly offensive content. I have structured the assignment in a way that the code below should be interchangeable for both datasets.

Please follow the instructions in the cell below to load and process whichever dataset you would like to use. You can select multiple lines and use `Control + Shift + C` to comment or uncomment them. 
```{r loading data, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
## Uncomment and run all lines below to load and clean the Friends dataset
# install.packages("friends")
# library(friends)
# data <- friends %>% select(season, episode, speaker, text) %>% drop_na() %>%
#   filter(speaker != "Scene Directions")
# show <- "Friends"
# main.characters <- c("Chandler Bing", "Joey Tribbiani", "Monica Geller", "Rachel Green", "Ross Geller", "Phoebe Buffay")

## Uncomment and run all lines below to load and clean the South Park dataset
## Note there is a possibility the line below will return an error (sometimes Github's servers will refuse the connection). If this happens just wait a minute and try again.
# data <- read.csv(url("https://raw.githubusercontent.com/BobAdamsEE/SouthParkData/master/All-seasons.csv"))
# colnames(data) <- c("season", "episode", "speaker", "text")
# data$season <- as.numeric(data$season)
# data <- drop_na(data) %>% as_tibble()
# show <- "South Park"
# main.characters <- c("Cartman", "Kenny", "Kyle", "Stan")
```

### Downsampling data (Optional)
I encourage you to attempt this assignment using the full version of whichever dataset you use. If you find that your computer is struggling due to the size of the data (crashing, overheating, running out of memory, etc.), then uncomment and run the cell below to take a random sample of the data. You may change `n` to be smaller or larger as necessary, but we warned that very small samples may render much of the following analysis meaningless. You may also sub-sample later on for specific parts of the assignment, just make sure to comment your code to make it clear if you are doing this.

*Even if you plan to use the full dataset, I would recommend using a sub-sample for testing your code before using the full sample.*
```{r sampling, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
#n <- 20000
#data <- sample_n(data, n)
```

### Questions
Q1: Complete the arguments for `unnest_tokens` to count all of the words in the show by episode.
```{r q1, echo=TRUE, tidy=TRUE}
words <- data %>% unnest_tokens()

words %>% count(word, sort = TRUE) %>%
  slice(1:10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + geom_col() +
  labs(y = NULL, x='Term frequency', title=paste("10 most frequent terms in ", show))
```
What are the top three most frequent words?

Q2: Modify the stopwords set to only use the snowball set of stopwords then modify words `words` to remove frequent stopwords.
```{r q2, echo=TRUE, tidy=TRUE}
data(stop_words) 
stop_words <- ### Your code here

words.2 <- words %>% ### Your code here

words.2 %>% count(word, sort = TRUE) %>%
  slice(1:10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + geom_col() +
  labs(y = NULL, x='Term frequency', title=paste("10 most frequent terms in ", show), caption = "Stopwords removed.")
```
What do you notice about the results? Is there anything else we might want to add to the stopword list?

Q3. Let's analyze how the language used by each character varies and how this varies over time. Complete the `count` and `group_by` statements to count the words used by each character in each season. Modify the code below to select a character of your choice (currently it picks one of the main characters at random.)
```{r q3, echo=TRUE, tidy=TRUE}
speaker.season.counts <- data %>% unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count() ### Add arguments

top_terms <- 
  speaker.season.counts %>%
  group_by() %>% ### Add arguments
  top_n(5, n) %>%
  ungroup() %>%
  arrange(word, -n)

X <- sample(main.characters, size=1) ## Replace with a character of your choosing
top_terms %>% filter(speaker == X) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = factor(season))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ season, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + labs(y="Word frequency", x="Term", title = paste("Top 5 words used by", X, "in each season of", show))
```

Q4. Modify the filter to look at another character. Do you notice any differences?
```{r q4, echo=TRUE, tidy=TRUE}
Y <- sample(main.characters, size=1) ### Set Y to equal a character of your choice
top_terms %>% filter(speaker == Y) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = factor(season))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ season, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + labs(y="Word frequency", x="Term", title = paste("Top 5 words used by", Y, "in each season of", show))
```

Q5. It should be apparent that there are a lot of different characters in these scripts, many of whom are minor characters appearing infrequently. Let's remove some of these characters before proceeding with the analysis. Complete the `summarize` function to create two measures: the number of seasons each character appears in and the total number of lines the character uttered in all seasons. Next, use the `filter` argument to remove infrequent characters by using these two variables. You can decide an appropriate cut-off to use in the filter.
```{r q5, echo=TRUE, tidy=TRUE}
frequent.characters <- speaker.season.counts %>% group_by(speaker) %>%
  summarize(seasons = , line_count = ) %>% ### Complete the arguments for seasons and line_count
  filter(line_count >= 100 & seasons >= 10) ### Add the filter

speaker.season.counts.adjusted <- speaker.season.counts %>%
  filter(speaker %in% frequent.characters$speaker)
```

Q6. Let's directly compare how similarly characters use language. Let's create a TF-IDF weighted document-term matrix, where each document is a character-season pair. The first line in the code below creates total word counts, which we will use to filter out infrequently occurring words. We then join these counts to our speaker-season dataset. The `unite` command can be used to create a new variable called `speaker_season` by concatenating the speaker and season columns. Please complete this command, making sure to retain the original columns. We will use this variable below.

Finally, we use `bind_tf_idf` to create the TF-IDF scores. There is no need to modify this last line.
```{r q6, echo=TRUE, tidy=TRUE}
word.totals <- speaker.season.counts.adjusted %>% 
  group_by(word) %>% 
  summarize(total = sum(n))

speaker.season <- left_join(speaker.season.counts.adjusted, word.totals) %>%
  unite("speaker_season") #### Add arguments to unite

tfidf <- speaker.season %>%  filter(total >= 10) %>% 
  bind_tf_idf(word, speaker_season, n)
```

Q7. By weighting terms by their TF-IDF scores, where a document is treated as all the lines by a character in a given season, we can better distinguish the language unique to particular characters. Run the chunk below to look at the same characters as above then answer the questions below.
```{r q7, echo=TRUE, tidy=TRUE}
top_tfidf <- 
  tfidf %>%
  group_by(speaker, season) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  arrange(word, -tf_idf)

top_tfidf %>% filter(speaker == X) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = factor(season))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ season, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + 
  labs(y="TF-IDF score", x="Term", 
                      title = paste("Top 5 words used by", X, "in each season of", show))

top_tfidf %>% filter(speaker == Y) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = factor(season))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ season, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + 
  labs(y="TF-IDF score", x="Term", 
                      title = paste("Top 5 words used by", Y, "in each season of", show))
```
How do the results vary now that we have used TF-IDF weighting? Do you think this is an appropriate way to measure documents?

We can use this data to construct a TF-IDF weighted document-term matrix (DTM). We then use the same code as shown in lecture to normalize each column and construct a cosine-similarity matrix via matrix multiplication.
```{r dtm, echo=TRUE, tidy=TRUE}
### There is no need to modify this cell.
# Construct DTM
M <- tfidf %>%
  cast_dtm(speaker_season, word, tf_idf) %>% as.matrix()

# Normalize columns
for (i in 1:dim(M)[1]) { 
  M[i,] <- M[i,]/sqrt(sum(M[i,]^2))
}

# Get cosine-similarity matrix
sims <- M %*% t(M)
```

Q8. Using the similarity matrix, find the 10 most similar character-season pairs. Print each pair and the similarity score. 

To exclude self-self pairs we can use the `diag` command to set the diagonal entries of the matrix to 0.
```{r q8, echo=TRUE, tidy=TRUE}
diag(sims) <- 0 ### Your code below
```
Which character-season pairs are most similar? 

# *Part 2: Word embeddings*

Q9. Use the `word2vec` function to train an embedding model using the *entire corpus*. Complete the function to use the skip-gram model with 100 dimensional vectors, a window size of 10, and 5 negative samples. Set the minimum word count to equal 10.

To speed things up we will set `threads = detectCores()` to run the process in parallel.
```{r q9, echo=TRUE, tidy=TRUE}
set.seed(10980)

print(detectCores()) # This will show how many cores you have
model <- word2vec::word2vec(x = tolower(data$text), # Do not modify
                  ### Add additional arguments and values here
                  iter=10L, # Do not modify
                  threads = detectCores())
```

Let's analyze how characters are represented in this model. This line will show the most similar words to the name of each of the main characters in the show. 

Note the `first.names` manipulation is necessary for Friends but is redundant for South Park because characters are only refered to by first names.
```{r q9, echo=TRUE, tidy=TRUE}
first.names <- main.characters %>% str_split(pattern = " ") %>% map(1) %>% unlist()
predict(model, tolower(first.names), type = "nearest", top_n = 10)
```
What do you notice about the results? Pick two characters and discuss the implications of these results.

Q10. We can use this embedding model to construct representations of each character's language use. Add the arguments for the `doc2vec` function to embed all of the lines by the main characters in the first season of the show.
```{r q10, echo=TRUE, tidy=TRUE}
data.filtered <- data %>% filter(speaker %in% main.characters & season ==1)
data.filtered$doc_id <- 1:dim(data.filtered)[1]
line.embedding <- doc2vec() ### Complete the argument
```

Q11. We now have a table containing the document embedding for each time. We now want to take the average of these embeddings for all lines by each character. Complete the line to define the object called `character.embeddings`. Hint: You will need to group the data by character and apply summarizing operation to all columns. The result should contain a single vector for character, representing the average of all of their line vectors. Note that some rows are missing from `line.embedding` because none of the words in the line were contained in the embedding vocabulary.
```{r q11, echo=TRUE, tidy=TRUE}
emb <- as.data.frame(line.embedding) # Converting the results to a data.frame
emb$speaker <- data.filtered$speaker # Adding a column with the speaker name
character.embeddings <- emb %>% ### Your code here
```

Q12. Now we can extract these character vectors and compute the similartity between character embeddings.

We can use a heatmap to visualize the similarities between characters. To do this we use the `melt` function from the `reshape2` package to transform the similarity matrix into a dataframe where each cell is now represented as a separate row, along with the names of each character in the pair. 

The diagonal values are set to be missing, otherwise they interfere with the shading of the color palette. 
```{r q12, echo=TRUE, tidy=TRUE}
### Do not modify any code here, just answer questions below after running it
M2 <- character.embeddings %>% select_if(is.numeric) %>% as.matrix()

for (i in 1:dim(M2)[1]) {
  M2[i,] <- M2[i,]/sqrt(sum(M2[i,]^2))
}

sims2 <- M2 %*% t(M2)
colnames(sims2) <- character.embeddings$speaker
rownames(sims2) <- colnames(sims2)
diag(sims2) <- NA

melted <- melt(sims2)
colnames(melted) <- c("i", "j", "sim")

ggplot(melted, aes(x=i, y=j, fill=sim)) + 
  geom_tile() + scale_fill_viridis_c()
```
Do these results give you any insight into the characters? Which characters appear to be similar? Are there any who appear to be distinctive from the others?

# *Part 3: Topic models*

For the final part of this assignment, you will train a topic model on the corpus and analyze the results. You will use a structural topic model with a prevalence covariate to allow topics to vary by the season of the show.

Let's start by pre-processing and preparing the documents using the functions supplied in the `stm` package.

Q13
Modify the function to use the same set of stopwords as used in the analysis above. You will also need to modify the other stopwords parameter, otherwise the model will remove the stopwords above and those in the preset lexicon.
```{r q13, echo=TRUE, tidy=TRUE}
meta <- data %>% select(season, speaker)
processed.docs <- textProcessor(data$text, metadata = meta) ### Add arguments related to stopwords
output <- prepDocuments(processed.docs$documents, processed.docs$vocab, processed.docs$meta, lower.thresh = 10)
```

Q14. Complete the `stm` function to run an initial topic modeling. Pick a value for `K` and modify the relevance argument to consist of a non-linear function of the season.
```{r q14, echo=TRUE, tidy=TRUE}
K= ### Choose a value of K
fit <- stm(documents = output$documents, vocab = output$vocab, 
           K=K,
           data = output$meta, 
           prevalence = , ### Specify the prevalence equation
           verbose = TRUE
           )
```

Q15. We can plot the topic proportions to get the topics that occur most frequently. 

We can extract these values by manipulating results of the `make.dt` function, which provides us with a vector of topic proportions for each document. Using `doc.props` and `doc.count` create a object creating the average proportion of each topic over all documents (the result should have the dimension `K x 1`). Transpose the result such that we have a single column with `K` values then find the top 5 highest proportion topics.
```{r q15, echo=TRUE, tidy=TRUE}
plot(fit, type = "summary")

doc.props <- make.dt(fit) # gets document proportions
doc.count <- dim(doc.props)[1] # gets number of documents
top5 <- doc.props %>% ### Your code here
print(top5)
```

Q16. Explore these five topics using any of the functions covered in lecture or in the `stm` documentation (e.g. `findThoughts`, `labelTopics`) then answer the questions below.
```{r q16, echo=TRUE, tidy=TRUE}
### Your code here
```
Name and describe each of the five topics.

  1. Name: Description:
  2. Name: Description:
  3. Name: Description:
  4. Name: Description:
  5. Name: Description:
  
Q 17. Use the`estimateEffect` function covered in lecture to analyze the relationship between the covariate and the topic. Modify the first argument of the function to accept the same formula as used above.

Next, modify the `topics` argument of `plot` to select the five topics covered above and change the `custom.labels` argument to contain a character vector of the names you assigned to topics in the previous question.
```{r q17, echo=TRUE, tidy=TRUE}
prep <- estimateEffect(, fit, meta = output$meta) ## Complete function
plot(prep, "season", method = "continuous", topics = c(), model = fit, xaxt = "n", xlab = "Season", 
     labeltype = "custom", custom.labels = c())
```
What do you notice? Are these topics stable over time or do they vary by season? Do these results tell you anything about the storyline of the show?

Q18 (Optional) Run a second model with different parameters to see if you can make any improvements. You may modify any step of the pipeline, for example by altering the pre-processing or by changing `K`. You can use the `searchK` function to analyze different specifications if desired.
```{r q18, echo=TRUE, tidy=TRUE}

```
Were you able to find a better fitting model? How does this new model compare to the previous iteration?

----
This is the end of the assignment. Follow the submission instructions at the beginning of this document. The procedure is the same as for homework 1, but please read the note on line 48 carefully.